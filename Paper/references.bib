@article{newton3PC2022,
  author = {Islamov, Rustem and Qian, Xun and Hanzely, Slavomír and Safaryan, Mher and Richtárik, Peter},
  title = {Distributed Newton-Type Methods with Communication Compression and Bernoulli Aggregation},
  journal={arXiv preprint arXiv: 2206.03588},
  year = {2022}
}

@article{Doikov2021,
title={Minimizing Uniformly Convex Functions by Cubic Regularization of Newton Method},
author={Nikita Doikov and Yurii Nesterov},
journal={Journal of Ooptimization Theory and Applications},
year={2021}
}

@article{Ghosh2021ByzantineNewton,
  title={{Escaping Saddle Points in Distributed Newton's Method with Communication Efficiency and Byzantine Resilience}},
  author={Avishek Ghosh and Raj Kumar Maity and Arya Mazumdar and Kannan Ramchandran},
  journal={arXiv preprint arXiv:2103.09424},
  year={2020}
}

@article{Ghosh2020ByzantineNewton,
  title={{Distributed Newton Can Communicate Less and Resist Byzantine Workers}},
  author={Avishek Ghosh and Raj Kumar Maity and Arya Mazumdar},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{Karimireddy2022Byzantine,
  title={{Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing}},
  author={Sai Praneeth Karimireddy and Lie He and Martin Jaggi},
  journal={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{Karimireddy2021Byzantine,
  title={{Learning from History for Byzantine Robust Optimization}},
  author={Sai Praneeth Karimireddy and Lie He and Martin Jaggi},
  journal={International Conference on Machine Learning (ICML)},
  year={2021}
}

@article{Yang2021PP+nonIID,
  title={{Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning}},
  author={Haibo Yang and Minghong Fang and Jia Liu},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{Nadiradze2021Asynchronous,
  title={{Asynchronous decentralized SGD with quantized and local updates}},
  author={Giorgi Nadiradze and Amirmojtaba Sabour and Peter Davies and Shigang Li and Dan Alistarh},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{Feyzmahdavian2021Asynchronous,
  title={{Asynchronous iterations in optimization: New sequence results and sharper algorithmic guarantees}},
  author={Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal={arXiv preprint arXiv:2109.04522},
  year={2021}
}

@article{Kovalev2021Decentralized,
  title={A linearly convergent algorithm for decentralized optimization: sending less bits for free!},
  author={Dmitry Kovalev and Anastasia Koloskova and Martin Jaggi and Peter Richtárik and Sebastian U. Stich},
  journal={The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2021}
}

@article{Koloskova2019Decentralized,
  title={Decentralized stochastic optimization and gossip algorithms with compressed communication},
  author={Anastasia Koloskova and Sebastian Stich and Martin Jaggi},
  journal={Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3478–3487. PMLR},
  year={2019}
}

@article{Koloskova2020Decentralized,
  title={Decentralized Deep Learning with Arbitrary Communication Compression},
  author={Anastasia Koloskova and Tao Lin and Sebastian U Stich and Martin Jaggi},
  journal={International Conference on Learning Representations},
  year={2020}
}

@article{Mishchenko2022ProxSkip,
  author = {Konstantin Mishchenko and Grigory Malinovsky and Sebastian Stich and Peter Richtárik},
  title = {{ProxSkip: Yes! Local gradient steps provably lead to communication acceleration! Finally!}},
  journal = {39th International Conference on Machine Learning (ICML)},
  year = {2022}
}

@inproceedings{Threshold2021,
 author = {Li, Yuhang and Guo, Yufei and Zhang, Shanghang and Deng, Shikuang and Hai, Yongqing and Gu, Shi},
 pages = {23426--23439},
 title = {Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks},
 volume = {34},
 year = {2021}
}

@article{IOSFabbro2022,
  author = {Fabbro, Nicolò Dal and Dey, Subhrakanti and Rossi, Michele and Schenato, Luca},
  title = {A Newton-type algorithm for federated learning based on incremental Hessian eigenvector sharing},
  journal = {arXiv preprint arXiv: 2202.05800},
  year = {2022},
}


@article{Sun2019LAG,
  author = {Sun, J. and Chen, T. and Giannakis, G. and Yang, Z.},
  title = {{Communication-efficient distributed learning via lazily aggregated quantized gradients}},
  journal = {Advances in Neural Information Processing Systems, 32:3370–3380},
  year = {2019}
}

@article{Ghadikolaei2021LENA,
  author = {Ghadikolaei, H. S. and Stich, S. and Jaggi, M.},
  title = {{LENA: Communication-efficient distributed learning with self-triggered gradient uploads}},
  journal = {International Conference on Artificial Intelligence and Statistics, pp. 3943–3951. PMLR},
  year = {2021}
}

@article{Chen2018LAG,
  author = {Chen, T. and Giannakis, G. and Sun, T. and Yin, W.},
  title = {{LAG: Lazily aggregated gradient for communication-efficient distributed learning}},
  journal = {Advances in Neural Information Processing Systems},
  year = {2018}
}

@article{Karimireddy2019EFsignSGD,
  author = {Karimireddy, S. P. and Rebjock, Q. and Stich, S. and Jaggi, M.},
  title = {{Error feedback fixes SignSGD and other gradient compression schemes}},
  journal = {36th International Conference on Machine Learning (ICML)},
  year = {2019}
}

@article{Cen2020DSVRG,
  author = {Shicong Cen and Huishuai Zhang and Yuejie Chi and Wei Chen and Tie-Yan Liu},
  title = {Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data},
  journal = {IEEE TRANSACTIONS ON SIGNAL PROCESSING, volume 68},
  year = {2020}
}

@article{Lee2017DSVRG,
  author = {Jason D. Lee and Qihang Lin and Tengyu Ma and Tianbao Yang},
  title = {{Distributed Stochastic Variance Reduced Gradient Methods by Sampling Extra Data with Replacement}},
  journal = {Journal of Machine Learning Research, vilume 18, pages 1-43},
  year = {2017}
}

@article{Qian2021ECLK,
  author = {Xun Qian and Peter Richtárik and Tong Zhang},
  title = {{Error compensated distributed SGD can be accelerated}},
  journal = {35th Conference on Neural Information Processing Systems},
  year = {2021}
}

@article{Nadiradze2021ADQL-SGD,
  author = {Giorgi Nadiradze and Amirmojtaba Sabour and Peter Davies and Shigang Li and Dan Alistarh},
  title = {{Asynchronous decentralized SGD with quantized and local updates}},
  journal = {35th Conference on Neural Information Processing Systems},
  year = {2021}
}

@article{Tyurin2022Dasha,
  author = {Alexander Tyurin and Peter Richtárik},
  title = {Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization},
  journal = {arXiv preprint arXiv:2202.01268},
  year = {2022}
}

@article{Sahu2021threshold,
  author = {Atal Narayan Sahu and Aritra Dutta and Ahmed M. Abdelmoniem and Trambak Banerjee and Marco Canini and Panos Kalnis},
  title = {{Rethinking gradient sparsification as total error minimization}},
  journal = {35th Conference on Neural Information Processing Systems},
  year = {2021}
}

@article{richtarik3PC,
  author = {Richtárik, Peter and Sokolov, Igor and Fatkhullin, Ilyas and Gasanov, Elnur and Li, Zhize and Gorbunov, Eduard},
  title = {3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation},
  journal = {39th International Conference on Machine Learning (ICML)},
  year = {2022}
}

@article{qian2021basis,
    title={Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning},
    author={Xun Qian and Rustem Islamov and Mher Safaryan and Peter Richt{\'a}rik},
    journal={Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
    year={2022},
}

@article{EF21,
    title={EF21: A new, simpler, theoretically better, and practically faster error feedback},
    author={Peter Richtárik and Igor Sokolov and Ilyas Fatkhullin},
    journal={35th Conference on Neural Information Processing Systems},
    year={2021},
}


@InProceedings{PAGE,
  title = {{PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization}},
  author = {Zhize Li and Hongyan Bao and Xiangliang Zhang and Peter Richtárik},
  year = {2021},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)}
}

@article{davis2018,
  title={{Subgradient methods for sharp weakly convex functions}},
  author={Damek Davis and Dmitriy Drusvyatskiy and Kellie J MacPhee and Courtney Paquette},
  journal={Journal of Optimization Theory and Applications, 179 (3):962–982},
  year={2018}
}

@article{Lang2012,
    title={{Fundamentals of differential geometry}},
    author={Serge Lang},
    year={2012},
    journal={Springer Science and Business Media, volume 191}
}

@article{amigo2021,
  title= {{Amortized Implicit Differentiation for Stochastic Bilevel Optimization}},
  author = {Michael Arbel and Julien Mairal},
  year = {2021},
  journal= {arXiv preprint arXiv:2111.14580}
}

@InProceedings{pure_cd,
  title = {{Random extrapolation for primal-dual coordinate descent}},
  author = {Ahmet Alacaoglu and Olivier Fercoq and Volkan Cevher},
  year = {2020},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)}
}

@article{chambolle2016,
  title = {{An introduction to continuous optimization for imaging}},
  author = {Antonin Chambolle and Thomas Pock},
  year = {2016},
  journal={ Acta Numerica, Cambridge University Press (CUP)}
}

@article{FISTA2009,
  title = {{A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems}},
  author = {Amir Beck and Marc Teboulle},
  year = {2009},
  journal={ IMA Journal of Numerical Analysis 39(4), 2069–2095}
}

@article{fercoq2019,
  title = {{Adaptive restart of accelerated gradient methods under local quadratic growth condition}},
  author = {Olivier Fercoq and Zheng Qu},
  year = {2019},
  journal={SIAM Journal on Imaging Sciences, 2(1), 183–202}
}


@article{liang2016,
  title = {{Convergence rates with inexact non-expansive operators}},
  author = {Jingwei Liang and Jalal Fadili and Gabriel Peyr\'e},
  year = {2016},
  journal={Mathematical Programming 159(1-2), 403–434}
}

@article{chambolle2011,
  title = {{A first-order primal-dual algorithm for convex problems with applications to imaging}},
  author = {Antonin Chambolle and Thomas Pock},
  year = {2011},
  journal={Journal of mathematical imaging and vision 40(1), 120–145}
}


@article{fercoqQEB,
  title = {{Quadratic error bound of the smoothed gap and the restarted averaged primal-dual hybrid gradient}},
  author = {Fercoq, Olivier},
  url = {https://hal.archives-ouvertes.fr/hal-03228252},
  year = {2021}
}

@article{trandinh2018smooth,
      title={A Smooth Primal-Dual Optimization Framework for Nonsmooth Composite Convex Minimization}, 
      author={Quoc Tran-Dinh and Olivier Fercoq and Volkan Cevher},
      journal = {SIAM Journal on Optimization 28(1), 96–134},
      year={2018},

}

@Book{RockWets98,
  Title                    = {Variational Analysis},
  Author                   = {{R. Tyrrell} Rockafellar and Roger J.-B. Wets},
  Publisher                = {Springer Verlag},
  Year                     = {1998},

  Address                  = {Heidelberg, Berlin, New York}
}

@article{bolte2016error,
      title={From error bounds to the complexity of first-order descent methods for convex functions}, 
      author={J\'er\^ome Bolte and Trong Phong Nguyen and Juan Peypouquet and Bruce Suter},
  journal={Mathematical Programming 165(2), 471–507},
  year={2017}
}

@article{Griewank1981,
  title={The modification of Newton's method for unconstrained optimization by bounding cubic terms},
  author={Andreas Griewank},
  journal={Technical report, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Technical Report NA/12},
  year={1981}
}

@techreport{grace,
 abstract = {Powerful computer clusters are used nowadays to train complex deep neural networks (DNN) on large datasets. Distributed training workloads increasingly become communication bound. For this reason, many lossy compression techniques have been proposed to reduce the volume of transferred data. Unfortunately, it is difficult to argue about the behavior of compression methods, because existing work relies on inconsistent evaluation testbeds and largely ignores the performance impact of practical system configurations. In this paper, we present a comprehensive survey of the most influential compressed communication methods for DNN training, together with an intuitive classification (i.e., quantization, sparsification, hybrid and low-rank). We also propose a unified framework and API that allows for consistent and easy implementation of compressed communication on popular machine learning toolkits. We instantiate our API on TensorFlow and PyTorch, and implement 16 such methods. Finally, we present a thorough quantitative evaluation with a variety of DNNs (convolutional and recurrent), datasets and system configurations. We show that the DNN architecture affects the relative performance among methods. Interestingly, depending on the underlying communication library and computational cost of compression/decompression, we demonstrate that some methods may be impractical.},
 author = {Xu, Hang and Ho, Chen-Yu and Abdelmoniem, Ahmed M. and Dutta, Aritra and Bergou, El Houcine and Karatsenidis, Konstantinos and Canini, Marco and Kalnis, Panos},
 title = {{Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation}},
 institution = {KAUST},
 url = {http://hdl.handle.net/10754/662495},
 year = {2020},
 month = {Apr} 
}

@article{scipy,
  author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title =     {{SciPy}: Open source scientific tools for {Python}},
  year =      {2001--},
  journal = "http://www.scipy.org/"
}

@article{EC-LSVRG,
	title={Error Compensated Loopless SVRG, Quartz, and SDCA for Distributed Optimization},
	author={Qian, Xun and Dong, Hanze and Richt{\'a}rik, Peter and Zhang, Tong},
	journal={arXiv preprint arXiv:2109.10049},
	year={2021}
}


@INPROCEEDINGS{CompressesDINGO2020,
  author={Ghosh, Avishek and Maity, Raj Kumar and Mazumdar, Arya and Ramchandran, Kannan},
  booktitle={2020 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Communication Efficient Distributed Approximate Newton Method}, 
  year={2020},
  volume={},
  number={},
  pages={2539-2544},
  doi={10.1109/ISIT44484.2020.9174216}
}

@article{Newton-MR2019,
    title  = {{Newton-MR: Newton's Method Without Smoothness or Convexity}},
    author = {Fred Roosta and Yang Liu and Peng Xu and Michael W. Mahoney},
    year   = {2019},
    journal= {arXiv preprint arXiv:1810.00303}
}

@article{HessianXvector1994,
  title={Fast Exact Multiplication by the Hessian},
  author={Barak A. Pearlmutter},
  journal={Neural Computation},
  year={1994}
}


@article{quasiNewton1974,
  title={A characterization of superlinear convergence and its application to quasi-Newton methods},
  author={J. E. Dennis and J. J. Mor\'{e}},
  journal={Mathematics of Computation, 28(126)},
  year={1974},
  pages={549–560}
}

@article{InexactNewton1982,
  title={Inexact Newton Methods},
  author={Ron S. Dembo and Stanley C. Eisenstat and Trond Steihaug},
  journal={SIAM J. Numer. Anal., 19(2)},
  year={1982},
  pages={400-408}
}

@article{Lin2014LargescaleLR,
  title={Large-scale logistic regression and linear support vector machines using spark},
  author={Chieh-Yen Lin and Cheng-Hao Tsai and Ching-pei Lee and Chih-Jen Lin},
  journal={2014 IEEE International Conference on Big Data (Big Data)},
  year={2014},
  pages={519-528}
}


@InProceedings{Zhuang2015,
author="Zhuang, Yong and Chin, Wei-Sheng and Juan, Yu-Chin and Lin, Chih-Jen",
editor="Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi",
title="Distributed Newton Methods for Regularized Logistic Regression",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="690--703",
abstract="Regularized logistic regression is a very useful classification method, but for large-scale data, its distributed training has not been investigated much. In this work, we propose a distributed Newton method for training logistic regression. Many interesting techniques are discussed for reducing the communication cost and speeding up the computation. Experiments show that the proposed method is competitive with or even faster than state-of-the-art approaches such as Alternating Direction Method of Multipliers (ADMM) and Vowpal Wabbit (VW). We have released an MPI-based implementation for public use.",
isbn="978-3-319-18032-8"
}



@InProceedings{DiSCO2015,
  author    = {Yuchen Zhang and Lin Xiao},
  title     = {{DiSCO: Distributed optimization for self-concordant empirical loss}},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, PMLR, volume 37, pages 362–370},
  year      = {2015},
}

@article{FedNL2021,
    title  = {{FedNL: Making Newton-Type Methods Applicable to Federated Learning}},
    author = {Mher Safaryan and Rustem Islamov and Xun Qian and Peter Richt\'{a}rik},
    year   = {2022},
    journal= {39th International Conference on Machine Learning (ICML)},
}

@InProceedings{LocalNewton2021,
  author    = {Vipul Gupta and Avishek Ghosh and Michal Derezinski and Rajiv Khanna and Kannan Ramchandran and Michael Mahoney},
  title     = {{LocalNewton: Reducing Communication Bottleneck for Distributed Learning}},
  booktitle = {37th Conference on Uncertainty in Artificial Intelligence (UAI 2021)},
  year      = {2021},
}


@InProceedings{Alimisis2021QNewton,
  author    = {Foivos Alimisis and Peter Davies and Dan Alistarh},
  title     = {Communication-Efficient Distributed Optimization with Quantized Preconditioners},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2021},
}


@INPROCEEDINGS{DAN-LA2020,  author={Zhang, Jiaqi and You, Keyou and Başar, Tamer},  booktitle={2020 59th IEEE Conference on Decision and Control (CDC)},   title={Achieving Globally Superlinear Convergence for Distributed Optimization with Adaptive Newton Method},   year={2020},  volume={},  number={},  pages={2329-2334},  doi={10.1109/CDC42340.2020.9304321}}

@InProceedings{GIANT2018,
  author    = {Shusen Wang and Fred Roosta abd Peng Xu and Michael W Mahoney},
  title     = {{GIANT}: Globally improved approximate {N}ewton method for distributed optimization},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
}


@Article{PN2006-cubic,
  author  = {Nesterov, Yurii and Polyak, Boris T.},
  title   = {Cubic regularization of {N}ewton method and its global performance},
  journal = {Mathematical Programming},
  year    = {2006},
  volume  = {108},
  number  = {1},
  pages   = {177--205},
}

@InProceedings{StichNIPS2018-memory,
  author    = {Stich, S. U.  and Cordonnier, J.-B.  and Jaggi, M.},
  title     = {Sparsified {SGD} with memory},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
}

@InCollection{stich2018sparse,
  author    = {Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  title     = {Sparsified {SGD} with Memory},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {4452--4463},
}

@InProceedings{1bit,
  author    = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  title     = {1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
  year      = {2014},
}

@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily,  Raef and Smith,  Adam and Thakurta,  Abhradeep},
  booktitle={2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}

@article{reddi2020adaptive,
    title  = {Adaptive Federated Optimization},
    author = {Sashank Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone\v{c}n\'{y} and Sanjiv Kumar and H. Brendan McMahan},
    year   = {2020},
    journal= {arXiv preprint arXiv:2003.00295},
}

@article{xie2019local,
  title={Local {A}da{A}lter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates},
  author={Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil and Lin, Haibin},
  journal={arXiv preprint arXiv:1911.09030},
  year={2019}
}

@Article{IntSGD,
  author  = {Konstantin Mishchenko and Bokun Wang and Dmitry Kovalev and Peter Richt\'{a}rik},
  journal = {arXiv preprint arXiv:2102.08374},
  title   = {{I}nt{SGD}: {F}loatless compression of stochastic gradients},
  year    = {2021},
}


@InProceedings{MM2019,
  author    = {Malitsky,  Yura and Mishchenko,  Konstantin},
  booktitle = {International Conference on Machine Learning (ICML)},
  title     = {Adaptive gradient descent without descent},
  year      = {2019},
}

@Article{biased2020,
  author  = {Aleksandr Beznosikov and Samuel Horv\'{a}th and Peter Richt\'{a}rik and Mher Safaryan},
  title   = {On Biased Compression for Distributed Learning},
  journal = {arXiv preprint arXiv:2002.12410},
  year    = {2020},
}

@inproceedings{khodak2019adaptive,
  title={Adaptive gradient-based meta-learning methods},
  author={Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@Article{OptClientSampling2020,
  author  = {Wenlin Chen and Samuel Horv\'{a}th and Peter Richt\'{a}rik},
  journal = {arXiv preprint arXiv:2010.13723},
  title   = {Optimal client sampling for federated learning},
  year    = {2020},
}

@InProceedings{ADIANA,
  author    = {Zhize Li and Dmitry Kovalev and Xun Qian and Peter Richt\'{a}rik},
  booktitle = {International Conference on Machine Learning},
  title     = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization},
  year      = {2020},
}


@InProceedings{MARINA,
  title = 	 {MARINA: Faster Non-Convex Distributed Learning with Compression},
  author =       {Gorbunov, Eduard and Burlachenko, Konstantin P. and Li, Zhize and Richtarik, Peter},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021}
}


@book{FOM-in-DS,
title={Distributed Learning Systems with First-Order Methods},
publisher={Foundations and Trends in Databases},
author={Liu, Ji and Zhang, Ce},
year={2020},
volume={9},
pages={1--100},
doi={10.1561/1900000062}
}

@article{DAN,
title = {Distributed adaptive {N}ewton methods with globally superlinear convergence},
author = {Jiaqi Zhang and Keyou You and Tamer Basar},
journal = {arXiv preprint arXiv:2002.07378},
year = {2020}
}


@inproceedings{DINGO,
title = {DINGO: Distributed Newton-type method for gradient-norm optimization},
author = {Rixon Crane and Fred Roosta},
booktitle = {Advances in Neural Information Processing Systems},
pages = {9498-9508},
volume = {32},
year = {2019}
}


@inproceedings{DANE,
title = {Communication-effcient distributed optimization using an approximate Newton-type method},
author = {Ohad Shamir and Nati Srebro and Tong Zhang},
booktitle = {Proceedings of the 31th International Conference on Machine Learning},
pages = {1000-1008},
volume = {32},
year = {2014}
}


@article{Islamov2021NewtonLearn,
title = {Distributed Second Order Methods with Fast Rates and Compressed Communication},
author = {Rustem Islamov and Xun Qian and Peter Richt{\'a}rik},
journal = {International Conference on Machine Learning (ICML)},
year = {2021}
}


@conference{Gorbunov2020localSGD,
title = {Local {SGD}: {U}nified theory and new efficient methods},
author = {Eduard Gorbunov and Filip Hanzely and Peter Richt{\'a}rik},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
year = {2021}
}


@inproceedings{CKMT-fedkashin,
title = {Expanding the Reach of Federated Learning by Reducing Client Resource Requirements},
author = {Caldas,  Sebastian and Kone\v{c}n\'{y},  Jakub and McMahan,  H.  Brendan and Talwalkar,  Ameet},
booktitle = {arXiv preprint arXiv:1812.07210v2},
year = {2019}
}


@InProceedings{Stich-localSGD,
  author    = {Stich, Sebastian U.},
  title     = {Local {SGD} Converges Fast and Communicates Little},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
}



@inproceedings{Candes-Tao-1,
title = {Decoding by linear programming},
author = {E. J. Cand\`{e}s and Tao, T.},
booktitle = {IEEE Transactions on Information Theory},
year = {2005},
volume = {51},
issue = {12}
}

@inproceedings{Candes-Tao-2,
title = {Near-optimal signal recovery from random projections and universal encoding strategies},
author = {Cand\`{e}s,  Emmanual and Tao, Terrence},
booktitle = {IEEE Transactions on Information Theory},
year = {2006},
volume = {52},
issue = {12}
}


@book{Ledoux-MC,
title={The Concentration of Measure Phenomenon},
author={Ledoux,  Michel},
place={Cambridge},
publisher={American Mathematical Society},
year={2001}
}

@article{Milman-MC,
author = {Giannopoulos, A. A.  and Milman, V.},
title = "Concentration property on probability spaces",
journal = "Advances in Mathematics",
volume = "156",
pages = "77-106",
year = "2000"
}



@inproceedings{LiDe,
title = {Dimension-Free Bounds for Low-Precision Training},
author = {Li, Zheng and De Sa, Christopher},
booktitle = {Advances in Neural Information Processing Systems 32},
year = {2019}
}

@article{Koch2,
title = "Constructive approximation of a ball by polytopes",
journal = "Mathematica Slovaca",
volume = "44",
number = "1",
pages = "99-105",
year = "1994",
issn = "0139-9918",
url = "https://eudml.org/doc/34376",
author = "Martin Kochol"
}

@article{Koch1,
title = "A note on approximation of a ball by polytopes",
journal = "Discrete Optimization",
volume = "1",
number = "2",
pages = "229 - 231",
year = "2004",
issn = "1572-5286",
doi = "https://doi.org/10.1016/j.disopt.2004.07.003",
url = "http://www.sciencedirect.com/science/article/pii/S1572528604000295",
author = "Martin Kochol",
keywords = "Approximation, Unit ball, Polytope, Volumetric cutting plane algorithm",
abstract = "We study properties of polytopes circumscribed by a unit sphere in Rn with either m extreme points or m facets. We show that if one measures the quality of approximation using the radius of an inscribing sphere then asymptotically the best-possible results are the same for both cases. Somewhat surprisingly, however, the volume can grow substantially faster in m for the case where the polytope has m facets."
}

@article{HHHSCR,
 abstract = {Due to their hunger for big data, modern deep learning models are trained in parallel, often in distributed environments, where communication of model updates is the bottleneck. Various update compression (e.g., quantization, sparsification, dithering) techniques have been proposed in recent years as a successful tool to alleviate this problem. In this work, we introduce a new, remarkably simple and theoretically and practically effective compression technique, which we call natural compression (NC). Our technique is applied individually to all entries of the to-be-compressed update vector and works by randomized rounding to the nearest (negative or positive) power of two. NC is ``natural'' since the nearest power of two of a real expressed as a float can be obtained without any computation, simply by ignoring the mantissa. We show that compared to no compression, NC increases the second moment of the compressed vector by the tiny factor 9/8 only, which means that the effect of NC on the convergence speed of popular training algorithms, such as distributed SGD, is negligible. However, the communications savings enabled by NC are substantial, leading to 3-4x improvement in overall theoretical running time. For applications requiring more aggressive compression, we generalize NC to natural dithering, which we prove is exponentially better than the immensely popular random dithering technique. Our compression operators can be used on their own or in combination with existing operators for a more aggressive combined effect. Finally, we show that NC is particularly effective for the in-network aggregation (INA) framework for distributed training, where the update aggregation is done on a switch, which can only perform integer computations.},
 author = {Horváth, Samuel and Ho, Chen-Yu and Horváth, Ľudovít and Sahu, Atal Narayan and Canini, Marco and Richtárik, Peter},
 journal = {CoRR},
 month = {May},
 title = {Natural Compression for Distributed Deep Learning},
 url = {http://arxiv.org/abs/1905.10988},
 volume = {abs/1905.10988},
 year = {2019}
}


@article{LyVe,
 author = {Lyubarskii, Yurii and Vershynin, Roman},
 title = {Uncertainty Principles and Vector Quantization},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {July 2010},
 volume = {56},
 number = {7},
 month = jul,
 year = {2010},
 issn = {0018-9448},
 pages = {3491--3501},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/TIT.2010.2048458},
 doi = {10.1109/TIT.2010.2048458},
 acmid = {1840529},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Frame representations, Kashin's representations, frame representations, kashin's representations, restricted isometries, uncertainty principles},
} 

@Book{UP-Harmonic1994,
  title     = {The Uncertainty Principle in Harmonic Analysis},
  publisher = {Springer-Verlag},
  year      = {1994},
  author    = {Havin, V. and J\"{o}ricke, B.},
}

@Article{Gabor1946,
  author  = {Gabor, D},
  title   = {Theory of Communication},
  journal = {Journal of the Institute of Electrical Engineering},
  year    = {1946},
  volume  = {93},
  pages   = {429--457},
}

@Article{Heisenberg1927,
  author  = {Heisenberg, Werner},
  title   = {\"{U}ber den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik},
  journal = {Zeitschrift f\"{u}r Physik},
  year    = {1927},
  volume  = {43},
  number  = {3--4},
  pages   = {172--198},
}

@article{kashin,
 author = {Kashin, Boris S.},
 title = {Diameters of some finite-dimensional sets and classes of smooth functions},
 journal = {Jour. Izv. Akad. Nauk SSSR Ser. Mat.},
 volume = {41},
 number = {2},
 year = {1977},
 pages = {334--351},
 url = {http://mi.mathnet.ru/izv1805},
}




@inproceedings{BaHe,
title = {Dissecting {Adam}: The Sign, Magnitude and Variance of Stochastic Gradients},
author = {Balles, Lukas and Hennig, Philipp},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
pages = {404-413},
year = {2018}
}

@inproceedings{BWAA,
title = {sign{SGD}: Compressed Optimisation for Non-Convex Problems},
author = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
pages = {560-569},
volume =  {80},
publisher = {PMLR},
year = {2018}
}

@inproceedings{BZAA,
title = {sign{SGD} with majority vote is communication efficient and fault tolerant},
author = {Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2019}
}

@conference{BoLe,
title = {Large scale online learning},
author = {Bottou, L\'eon and Le Cun, Yann},
booktitle = {Advances in Neural Information Processing Systems},
year = {2003}
}

@conference{CCC,
title = {Stochastic Spectral Descent for Restricted Boltzmann Machines},
author = {Carlson, David and Cevher, Volkan and Carin, Lawrence},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {111-119},
year = {2015}
}

@conference{DHS,
title = {Adaptive subgradient methods for online learning and stochastic optimization},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
pages = {2121–2159},
year = {2011}
}

@conference{GHJY,
title = {Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition},
author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
booktitle = {JMLR: Workshop and Conference Proceedings},
pages = {1-46},
volume = {40},
year = {2015}
}

@conference{GhLa,
title = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
author = {Ghadimi, Saeed and Lan, Guanghui},
booktitle = {SIAM Journal on Optimization},
pages = {2341–2368},
volume = {23(4)},
year = {2013}
}

@Article{DCGD,
  author  = {Sarit Khirirat and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal = {arXiv preprint arXiv:1806.06573},
  title   = {Distributed learning with compressed gradients},
  year    = {2018},
}

@conference{KiBa,
title = {Adam: A method for stochastic optimization},
author = {Kingma, Diederik and Ba, Jimmy},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2015}
}

@conference{KSH,
title = {Imagenet classification with deep convolutional neural networks},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1097–1105},
year = {2012}
}

@conference{LHMWD,
title = {Deep gradient compression: Reducing the communication bandwidth for distributed training},
author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J.},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2018}
}

@conference{LCCH,
title = {sign{SGD} via zeroth-order oracle},
author = {Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2019}
}

@InProceedings{SQSM,
  title =    {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
  author =   {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {3252--3261},
  year =   {2019},
  volume =   {97}
}

@Article{stich2019,
  author  = {Sebastian U. Stich and Sai Praneeth Karimireddy},
  title   = {The Error-Feedback Framework: Better Rates for {SGD} with Delayed Gradients and Compressed Communication},
  journal = {arXiv preprint arXiv:1909.05350},
  year    = {2019},
}



@conference{QRGSLS,
title = {{SGD} with Arbitrary Sampling: General Analysis and Improved Rates},
author = {Qian, Xun and Richt\'arik, Peter and Gower, Robert Mansel and Sailanbayev, Alibek and Loizou, Nicolas and Shulgin, Egor},
booktitle = {International Conference on Machine Learning},
pages = {},
year = {2019}
}

@conference{RKK,
title = {On the convergence of {Adam} and beyond},
author = {Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2019}
}

@conference{RiBr,
title = {A direct adaptive method for faster backpropagation learning: The {Rprop} algorithm},
author = {Riedmiller, Martin and Braun, Heinrich},
booktitle = {IEEE International Conference on Neural Networks},
pages = {586-591},
year = {1993}
}

@conference{RoMo,
title = {A Stochastic Approximation Method},
author = {Robbins, Herbert and Monro, Sutton},
booktitle = {The Annals of Mathematical Statistics},
pages = {400-407},
volume = {22(3)},
year = {1951}
}


@conference{SFDLY,
title = {1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech {DNN}s},
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
pages = {},
year = {2014}
}

@conference{Shev,
title = {On the absolute constants in the Berry–Esseen type inequalities for identically distributed summands},
author = {Shevtsova, Irina},
booktitle = {arXiv preprint arXiv:1111.6554},
pages = {},
year = {2011}
}

@conference{SRB,
title = {Minimizing finite sums with the stochastic average gradient},
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
booktitle = {Mathematical Programming},
pages = {83–112},
volume = {162(1-2)},
year = {2017}
}

@conference{Strom,
title = {Scalable distributed {DNN} training using commodity {GPU} cloud computing},
author = {Strom, Nikko},
booktitle = {Sixteenth Annual Conference of the International Speech Communication Association},
pages = {},
year = {2015}
}

@conference{TiHi,
title = {{RMSprop}},
author = {Tieleman, Tijmen and Hinton, Geoffrey E.},
booktitle = {Coursera: Neural Networks for Machine Learning, Lecture 6.5},
pages = {},
year = {2012}
}

@book{Versh,
place={Cambridge},
series={Cambridge Series in Statistical and Probabilistic Mathematics},
title={High-Dimensional Probability: An Introduction with Applications in Data Science},
DOI={10.1017/9781108231596},
publisher={Cambridge University Press},
author={Vershynin, Roman},
year={2018},
collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@conference{VBS,
title = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models (and an Accelerated Perceptron)},
author = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, PMLR},
pages = {},
volume = {89},
year = {2019}
}

@conference{WSLCPW,
title = {Atomo: Communication-efficient learning via atomic sparsification},
author = {Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
booktitle = {Advances in Neural Information Processing Systems},
pages = {},
year = {2018}
}

@conference{WXYWWCL,
title = {Terngrad: Ternary gradients to reduce communication in distributed deep learning},
author = {Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1509–1519},
year = {2017}
}

@conference{WRSSR,
title = {The marginal value of adaptive gradient methods in machine learning},
author = {Wilson, Ashia and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems},
pages = {4148-4158},
year = {2017}
}

@conference{ZRSKK,
title = {Adaptive methods for nonconvex optimization},
author = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Advances in Neural Information Processing Systems},
pages = {9815-9825},
year = {2018}
}

@conference{Zei,
author = {{Zeiler}, Matthew D.},
title = "{{ADADELTA}: An Adaptive Learning Rate Method}",
booktitle = {arXiv preprint arXiv:1212.5701},
year = "2012"
}


@conference{ZLKALZ,
title = {{ZipML}: Training linear models with end-to-end low precision, and a little bit of deep learning},
author = {Zhang, Hantian and Li, Jerry and Kara, Kaan and Alistarh, Dan and Liu, Ji and Zhang, Ce},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
pages = {4035–4043},
volume = {70},
year = {2017}
}


% ---------------------------------------------- initial
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}



% ---------------------------------------------- 4 Feb 2020
@Article{Nemirovski-Juditsky-Lan-Shapiro-2009,
  author  = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  title   = {Robust stochastic approximation approach to stochastic programming},
  journal = {SIAM Journal on Optimization},
  year    = {2009},
  volume  = {19},
  number  = {4},
  pages   = {1574--1609},
}

@InProceedings{SGD-AS,
  author    = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt\'{a}rik, Peter},
  title     = {{SGD}: General Analysis and Improved Rates},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {5200--5209},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
}

@InProceedings{sigma_k,
  author    = {Gorbunov,  Eduard and Hanzely,  Filip and Richt\'{a}rik,  Peter},
  title     = {A Unified Theory of {SGD}: {V}ariance Reduction,  Sampling,  Quantization and Coordinate Descent},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics},
  year      = {2020},
}

@InProceedings{Alistarh-SparsGradMethods2018,
  author    = {Dan Alistarh and Torsten Hoefler and Mikael Johansson and Sarit Khirirat and Nikola Konstantinov and C\'{e}dric Renggli},
  title     = {The Convergence of Sparsified Gradient Methods},
  booktitle = {32nd Conference on Neural Information Processing Systems},
  year      = {2018},
  journal   = {32nd Conference on Neural Information Processing Systems},
}

@InProceedings{errorSGD,
  author    = {Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
  title     = {Error Compensated Quantized {SGD} and its Applications to Large-scale Distributed Optimization},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  pages     = {5325--5333},
  address   = {Stockholmsmässan, Stockholm Sweden},
  month     = {10--15 Jul},
  publisher = {PMLR},
}

@Article{DIANA,
  author        = {Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  title         = {Distributed Learning with Compressed Gradient Differences},
  journal       = {arXiv preprint arXiv:1901.09269},
  year          = {2019},
}

@Article{DIANA-VR,
  author  = {Horv\'{a}th,  Samuel and Kovalev,  Dmitry and Mishchenko,  Konstantin and Stich,  Sebastian and Richt\'{a}rik,  Peter},
  title   = {Stochastic distributed learning with gradient quantization and variance reduction},
  journal = {arXiv preprint arXiv:1904.05115},
  year    = {2019},
}

@InProceedings{DoubleSqueeze2019,
  title =    {$\texttt{DoubleSqueeze}$: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression},
  author =   {Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {6155--6165},
  year =     {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  address =      {Long Beach, California, USA},
  month =    {09--15 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v97/tang19d/tang19d.pdf},
  url =      {http://proceedings.mlr.press/v97/tang19d.html},
  abstract =     {A standard approach in large scale machine learning is distributed stochastic gradient training, which requires the computation of aggregated stochastic gradients over multiple nodes on a network. Communication is a major bottleneck in such applications, and in recent years, compressed stochastic gradient methods such as QSGD (quantized SGD) and sparse SGD have been proposed to reduce communication. It was also shown that error compensation can be combined with compression to achieve better convergence in a scheme that each node compresses its local stochastic gradient and broadcast the result to all other nodes over the network in a single pass. However, such a single pass broadcast approach is not realistic in many practical implementations. For example, under the popular parameter-server model for distributed learning, the worker nodes need to send the compressed local gradients to the parameter server, which performs the aggregation. The parameter server has to compress the aggregated stochastic gradient again before sending it back to the worker nodes. In this work, we provide a detailed analysis on this two-pass communication model, with error-compensated compression both on the worker nodes and on the parameter server. We show that the error-compensated stochastic gradient algorithm admits three very nice properties: 1) it is compatible with an \emph{arbitrary} compression technique; 2) it admits an improved convergence rate than the non error-compensated stochastic gradient method such as QSGD and sparse SGD; 3) it admits linear speedup with respect to the number of workers. The empirical study is also conducted to validate our theoretical results.}
}



@Article{ASDA,
  author        = {Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
  title         = {Stochastic reformulations of linear systems: algorithms and convergence theory},
  journal       = {arXiv preprint arXiv:1706.01108},
  year          = {2017},
}

@inproceedings{tonko,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1306--1316},
  year={2018}
}


@inproceedings{terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1509--1519},
  year={2017}
}



@inproceedings{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, C{\'e}dric},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5977--5987},
  year={2018}
}


@InProceedings{zipml,
  author    = {Hantian Zhang and Jerry Li and Kaan Kara and Dan Alistarh and Ji Liu and Ce Zhang},
  title     = {{Z}ip{ML}: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  pages     = {4035--4043},
  address   = {International Convention Centre, Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
}

@article{switchML,
  author    = {Amedeo Sapio and Marco Canini and  Chen{-}Yu Ho and
               Jacob Nelson and
               Panos Kalnis and
               Changhoon Kim and
               Arvind Krishnamurthy and
               Masoud Moshref and
               Dan R. K. Ports and
               Peter Richt{\'{a}}rik},
  title     = {Scaling Distributed Machine Learning with In-Network Aggregation},
  journal   = {arXiv preprint arXiv:1903.06701},
  year      = {2019},
}

@InProceedings{deep,
  author    = {Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
  title     = {Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
  booktitle = {ICLR 2018 - International Conference on Learning Representations},
  year      = {2018},
}

@Article{Goyal2017:large,
  author        = {Priya Goyal and Piotr Doll{\'{a}}r and Ross B. Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  title         = {Accurate, Large Minibatch {SGD:} Training {ImageNet} in 1 Hour},
  journal       = {arXiv preprint arXiv:1706.02677},
  year          = {2017},
}

@Article{FEDOPT,
  author  = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richt\'{a}rik, Peter},
  journal = {arXiv preprint arXiv:1610.02527},
  title   = {Federated optimization: {D}istributed machine learning for on-device intelligence},
  year    = {2016},
}

@InProceedings{FEDLEARN,
  author        = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Yu, Felix and Richt\'{a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  title         = {Federated learning: strategies for improving communication efficiency},
  booktitle     = {NIPS Private Multi-Party Machine Learning Workshop},
  year          = {2016},
}

@Article{Reddi:2016aide,
  author  = {Sashank J. Reddi and Jakub Kone\v{c}n{\'y} and Peter Richt{\'a}rik and Barnab{\'a}s P{\'o}czos and Alexander J. Smola},
  title   = {{AIDE}: Fast and Communication Efficient Distributed Optimization},
  journal = {CoRR},
  year    = {2016},
  volume  = {abs/1608.06879},
}

@Article{Hydra,
  author  = {Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  title   = {Distributed coordinate descent method for learning with big data},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {75},
  pages   = {1--25},
}

@Article{Hydra2,
  author  = {Fercoq, Olivier and Qu, Zheng and Richt\'{a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  title   = {Fast distributed coordinate descent for minimizing non-strongly convex losses},
  journal = {IEEE International Workshop on Machine Learning for Signal Processing},
  year    = {2014},
}

@InProceedings{cocoa,
  author    = {Jaggi, Martin and Smith, Virginia and Tak{\'a}\v{c}, Martin and Terhorst, Jonathan and Krishnan, Sanjay and Hofmann, Thomas and Jordan, Michael I.},
  title     = {Communication-efficient distributed dual coordinate ascent},
  booktitle = {Advances in Neural Information Processing Systems 27},
  year      = {2014},
  url       = {http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf},
}

@InCollection{Mann2009:parallelSGD,
  author    = {Ryan McDonald and Mohri, Mehryar and Silberman, Nathan and Dan Walker and Gideon S. Mann},
  title     = {Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models},
  booktitle = {Advances in Neural Information Processing Systems 22},
  publisher = {Curran Associates, Inc.},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
  pages     = {1231--1239},
}

@InCollection{Zinkevich2010:parallelSGD,
  author    = {Martin Zinkevich and Markus Weimer and Li, Lihong and Alex J. Smola},
  title     = {Parallelized Stochastic Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 23},
  publisher = {Curran Associates, Inc.},
  year      = {2010},
  editor    = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
  pages     = {2595--2603},
}

@InProceedings{Shamir2014:approxnewton,
  author    = {Ohad Shamir and Nati Srebro and Tong Zhang},
  title     = {Communication-Efficient Distributed Optimization using an Approximate {N}ewton-type Method},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  year      = {2014},
  editor    = {Eric P. Xing and Tony Jebara},
  volume    = {32},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1000--1008},
  address   = {Bejing, China},
  publisher = {PMLR},
  abstract  = {We present a novel Newton-type method for distributed optimization,  which is particularly well suited for stochastic optimization and  learning problems.  For quadratic objectives, the method enjoys a  linear rate of convergence which provably \emphimproves with the  data size, requiring an essentially constant number of iterations  under reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages of our method compared to other  approaches, such as one-shot parameter averaging and ADMM.},
  file      = {shamir14.pdf:http\://proceedings.mlr.press/v32/shamir14.pdf:PDF},
}



@InProceedings{localSGD-AISTATS2020,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Tighter theory for local {SGD} on identical and heterogeneous data},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)},
  year      = {2020},
}


@InProceedings{Gupta:2015limited,
  author    = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  title     = {Deep Learning with Limited Numerical Precision},
  booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
  year      = {2015},
  series    = {ICML'15},
  pages     = {1737--1746},
  publisher = {JMLR.org},
  acmid     = {3045303},
  location  = {Lille, France},
  numpages  = {10},
}

@article{beck2009fista,
  title={A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  author={Beck, Amir and Teboulle, Marc},
  journal={SIAM Journal on Imaging Sciences},
  volume={2},
  number={1},
  pages={183--202},
  year={2009},
  publisher={SIAM}
}

@Book{Beck-book-nonlinear,
  author    = {Beck,  Amir},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {Introduction to Nonlinear Optimization: {T}heory,  Algorithms,  and Applications with {MATLAB}},
  year      = {2014},
  address   = {USA},
  isbn      = {1611973643},
}

@InProceedings{FL2017-AISTATS,
  author    = {McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Ag\"{u}era y Arcas, Blaise},
  title     = {Communication-efficient learning of deep networks from decentralized data},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year      = {2017},
}

@Article{FL-big,
  author  = {Kairouz et al, Peter},
  title   = {Advances and Open Problems in Federated Learning},
  journal = {arXiv preprint arXiv:1912.04977},
  year    = {2019},
}

@Article{FL_survey_2020,
  author  = {Li,  Tian and Sahu,  Anit Kumar and Talwalkar,  Ameet and Smith,  Virginia},
  journal = {IEEE Signal Processing Magazine},
  title   = {Federated learning: challenges, methods, and future directions},
  year    = {2020},
  number  = {3},
  pages   = {50--60},
  volume  = {37},
  doi     = {10.1109/MSP.2020.2975749},
}

@Article{FedRR,
  author  = {Konstantin Mishchenko and Ahmed Khaled and Peter Richt\'{a}rik},
  journal = {arXiv preprint arXiv:2102.06704},
  title   = {Proximal and federated random reshuffling},
  year    = {2021},
}

@inproceedings{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1200--1205},
  year={2017},
  organization={ACM}
}

@InProceedings{Vaswani2019-overparam,
  author    = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  title     = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models and an Accelerated Perceptron},
  booktitle = {22nd International Conference on Artificial Intelligence and Statistics},
  year      = {2019},
  volume    = {89},
  series    = {PMLR},
  pages     = {1195-1204},
}

@Article{L-SVRG-AS,
  author  = {Qian, Xun and Qu, Zheng and Richt\'{a}rik, Peter},
  title   = {L-SVRG and L-Katyusha with arbitrary sampling},
  journal = {arXiv preprint arXiv:1906.01481},
  year    = {2019},
}

@InProceedings{SN2019,
  author      = {Kovalev,  Dmitry and Mishchenko,  Konstanting and Richt\'{a}rik,  Peter},
  title       = {Stochastic {N}ewton and Cubic {N}ewton Methods with Simple Local Linear-Quadratic Rates},
  booktitle   = {NeurIPS Beyond First Order Methods Workshop},
  year        = {2019},
}

@InProceedings{L-SVRG,
  author    = {Kovalev,  Dmitry and Horv\'{a}th,  Samuel and Richt\'{a}rik,  Peter},
  title     = {Don’t jump through hoops and remove those loops: {SVRG} and {K}atyusha are better without the outer loop},
  booktitle = {Proceedings of the 31st International Conference on Algorithmic Learning Theory},
  year      = {2020},
  journal   = {arXiv preprint arXiv:1901.08689},
}

@Article{sign_descent_2019,
  author  = {Safaryan, Mher and Richt\'{a}rik, Peter},
  title   = {On stochastic sign descent methods},
  journal = {arXiv preprint arXiv:1905.12938},
  year    = {2019},
}

@InProceedings{Na2017:limitedprecision,
  author    = {T. Na and J. H. Ko and J. Kung and S. Mukhopadhyay},
  title     = {On-chip training of recurrent neural networks with limited numerical precision},
  booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
  year      = {2017},
  pages     = {3716-3723},
  month     = {May},
  doi       = {10.1109/IJCNN.2017.7966324},
  keywords  = {recurrent neural nets;RNN;gated recurrent unit;recurrent neural networks;stochastic rounding;target numeric format;Hardware;Logic gates;Quantization (signal);Recurrent neural networks;System-on-chip;Training;batch normalization;dynamic fixed point;limited precision training;recurrent neural network},
}

@InCollection{Alistarh2018:topk,
  author    = {Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, Cedric},
  title     = {The Convergence of Sparsified Gradient Methods},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {5977--5987},
}


@Article{RDME,
  author        = {Kone\v{c}n\'{y}, Jakub and Richt\'{a}rik, Peter},
  title         = {Randomized distributed mean estimation: accuracy vs communication},
  journal       = {Frontiers in Applied Mathematics and Statistics},
  year          = {2018},
  volume        = {4},
  number        = {62},
  pages         = {1--11},
}

@InProceedings{Suresh2017,
  author    = {Suresh, Ananda Theertha and Yu, Felix X. and Kumar, Sanjiv and McMahan, H. Brendan},
  title     = {Distributed Mean Estimation with Limited Communication},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
}



@InProceedings{Dryden2016:topk,
  author    = {N. Dryden and T. Moon and S. A. Jacobs and B. V. Essen},
  title     = {Communication Quantization for Data-Parallel Training of Deep Neural Networks},
  booktitle = {2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC)},
  year      = {2016},
  pages     = {1-8},
  month     = {Nov},
  doi       = {10.1109/MLHPC.2016.004},
  keywords  = {message passing;neural nets;parallel processing;HPC system;MNIST dataset;MPI Allreduce routine;adaptive quantization algorithm;communication quantization;data-parallel training;deep neural networks;high-performance computing infrastructure;one-bit quantization;quantizing gradient updates;threshold quantization;Bandwidth;Data models;Electronic mail;Laboratories;Parallel processing;Quantization (signal);Training},
}

@InProceedings{Aji2017:topk,
  author    = {Aji, Alham Fikri and Heafield, Kenneth},
  title     = {Sparse Communication for Distributed Gradient Descent},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  year      = {2017},
  pages     = {440--445},
  publisher = {Association for Computational Linguistics},
  location  = {Copenhagen, Denmark},
}

@book{bekkerman2011scaling,
  title={Scaling up machine learning: Parallel and distributed approaches},
  author={Bekkerman, Ron and Bilenko, Mikhail and Langford, John},
  year={2011},
  publisher={Cambridge University Press}
}

@inproceedings{recht2011hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={693--701},
  year={2011}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}



@article{SGD,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@incollection{jiang,
title = {A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication},
author = {Jiang, Peng and Agrawal, Gagan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {2525--2536},
year = {2018},
publisher = {Curran Associates, Inc.}
}

@article{khirirat2018distributed,
  title={Distributed learning with compressed gradients},
  author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  journal={arXiv preprint arXiv:1806.06573},
  year={2018}
}

@Article{Roberts1962:randdithering,
  author   = {L. Roberts},
  title    = {Picture coding using pseudo-random noise},
  journal  = {IRE Transactions on Information Theory},
  year     = {1962},
  volume   = {8},
  number   = {2},
  pages    = {145-154},
  month    = {February},
  issn     = {0096-1000},
  doi      = {10.1109/TIT.1962.1057702},
  keywords = {Image coding;PCM communication;Pseudonoise codes;Phase change materials;TV;Noise reduction;Quantization;Transmitters;Bandwidth;Pulse modulation;Decoding;Laboratories;Facsimile},
}

@Article{Goodall1951:randdithering,
  author  = {W. M. Goodall},
  title   = {Television by pulse code modulation},
  journal = {The Bell System Technical Journal},
  year    = {1951},
  volume  = {30},
  number  = {1},
  pages   = {33-49},
  month   = {Jan},
  issn    = {0005-8580},
  doi     = {10.1002/j.1538-7305.1951.tb01365.x},
}

@article{mishchenko2019distributed,
  title={Distributed Learning with Compressed Gradient Differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}

@InProceedings{EC-SGD,
  author    = {Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt\'{a}rik, Peter},
  booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
  title     = {Linearly Converging Error Compensated {SGD}},
  year      = {2020},
}

@article{diana2,
  title={Stochastic Distributed Learning with Gradient Quantization and Variance Reduction},
  author={Horv\'{a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt\'{a}rik, Peter},
  journal={arXiv preprint arXiv:1904.05115},
  year={2019}
}

@misc{gloo,
  key          = "gloo",
  author = {{Facebook}},
  title        = {{Gloo}},
  howpublished = "{\url{https://github.com/facebookincubator/gloo}}",
}

@misc{dpdk,
  key          = "dpdk",
  title        = {{DPDK}},
  howpublished = "{\url{https://www.dpdk.org/}}",
}

@misc{tfbench,
  key          = "tfbench",
  title        = {{TensorFlow benchmarks}},
  howpublished = "{\url{https://github.com/tensorflow/benchmarks}}",
}

@InProceedings{horvath2018nonconvex,
  author    = {Horv{\'a}th, Samuel and Richt\'{a}rik, Peter},
  title     = {Nonconvex Variance Reduced Optimization with Arbitrary Sampling},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {2781--2789},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  abstract  = {We provide the first importance sampling variants of variance reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of \texttt{SVRG}, \texttt{SAGA} and \texttt{SARAH}. Our methods have the capacity to speed up the training process by an order of magnitude compared to the state of the art on real datasets. Moreover, we also improve upon current mini-batch analysis of these methods by proposing importance sampling for minibatches in this setting. Surprisingly, our approach can in some regimes lead to superlinear speedup with respect to the minibatch size, which is not usually present in stochastic optimization. All the above results follow from a general analysis of the methods which works with \emph{arbitrary sampling}, i.e., fully general randomized strategy for the selection of subsets of examples to be sampled in each iteration. Finally, we also perform a novel importance sampling analysis of \texttt{SARAH} in the convex setting.},
}


@article{katharopoulos2018not,
  title={Not all samples are created equal: Deep learning with importance sampling},
  author={Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1803.00942},
  year={2018}
}

@inproceedings{defazio2014saga,
  title={{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1646--1654},
  year={2014}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={315--323},
  year={2013}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4700--4708},
  year={2017}
}

@article{lim20183lc,
  title={3LC: Lightweight and Effective Traffic Compression for Distributed Machine Learning},
  author={Lim, Hyeontaek and Andersen, David G and Kaminsky, Michael},
  journal={arXiv preprint arXiv:1802.07389},
  year={2018}
}

@article{99percent,
  title={99\% of Parallel Optimization is Inevitably a Waste of Time},
  author={Mishchenko, Konstantin and Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09437},
  year={2019}
}

@misc{nvidia-imagenet,
  key          = "nvidia",
  author = {{NVIDIA}},
  title        = {{nvidia}},
  howpublished = "{\url{https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5}}",
}

@book{arnold,
    title={A First Course in order Statistics},
    author={Barry C. Arnold, N. Balakrishnan and H. N. Nagaraja},
    year={1992},
    publisher={John Wiley and Sons Inc.}
}

@article{uniform,
author = {Moghadam, S.A. and Pazira, Hassan},
year = {2011},
month = {07},
pages = {719-723},
title = {The Relations Among the Order Statistics of Uniform Distribution},
volume = {6},
journal = {Trends in Applied Sciences Research},
doi = {10.3923/tasr.2011.719.723}
}

@book{gumbel,
    title={Statistical Theory of Extreme Values and Some Practical Applications},
    author={Emil J. Gumbel},
    year={1954},
    publisher={National Bureau of Standards Applied Mathematics Series}
}

@article{smallnormal,
author = "Godwin, H. J.",
doi = "10.1214/aoms/1177730036",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "06",
number = "2",
pages = "279--285",
publisher = "The Institute of Mathematical Statistics",
title = "Some Low Moments of Order Statistics",
url = "https://doi.org/10.1214/aoms/1177730036",
volume = "20",
year = "1949"
}

@article{smallnormal1,
author = "Moriguti, Sigeiti",
doi = "10.1214/aoms/1177729542",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "12",
number = "4",
pages = "523--536",
publisher = "The Institute of Mathematical Statistics",
title = "Extremal Properties of Extreme Value Distributions",
url = "https://doi.org/10.1214/aoms/1177729542",
volume = "22",
year = "1951"
}

@article{chi,
author = "Govindarajulu, Zakkula",
doi = "10.1214/aoms/1177704362",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "12",
number = "4",
pages = "1292--1305",
publisher = "The Institute of Mathematical Statistics",
title = "Exact Lower Moments of Order Statistics in Samples from the Chi- Distribution (1 d.f.)",
url = "https://doi.org/10.1214/aoms/1177704362",
volume = "33",
year = "1962"
}

@article{adis,
author = {Okoyo Collins Omondi},
year = {2016},
month = {07},
title = {Order Statistics of Uniform, Logistic and Exponential Distributions},
}

@article{fisher_tippett, title={Limiting forms of the frequency distribution of the largest or smallest member of a sample}, volume={24}, DOI={10.1017/S0305004100015681}, number={2}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, publisher={Cambridge University Press}, author={Fisher, R. A. and Tippett, L. H. C.}, year={1928}, pages={180–190}}

@Book{haan,
  author =  "L. de Haan and A. Ferreira",
  title =   "Extreme Value Theory. An Introduction",
  publisher =   "Springer",
  year =    "2006",
  address = "New York",
  annote =  "extremes",
}

@book{anbn,
author={Embrechts,Paul and Klüppelberg,Claudia and Mikosch,Thomas and SpringerLink (Online service)},
year={1997},
title={Modelling Extremal Events: for Insurance and Finance},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
volume={33},
language={English},
}

@book{gnedenko,
author={Gnedenko, B.V.},
year={1943},
title={Sur la distribution limit e du terme d’une s erieal eatoir},
publisher={Ann. Math.44},
volume={44},
pages = {423–453}
}

@misc{stich2019,
    title={The Error-Feedback Framework: Better Rates for {SGD} with Delayed Gradients and Compressed Communication},
    author={Sebastian U. Stich and Sai Praneeth Karimireddy},
    year={2019},
    eprint={1909.05350},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{vogels,
  author    = {Thijs Vogels and
               Sai Praneeth Karimireddy and
               Martin Jaggi},
  title     = {Power{SGD}: Practical Low-Rank Gradient Compression for Distributed Optimization},
booktitle  = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
  year      = {2019},
}

@InProceedings{sun,
author="Sun, Haobo
and Shao, Yingxia
and Jiang, Jiawei
and Cui, Bin
and Lei, Kai
and Xu, Yu
and Wang, Jiang",
editor="Li, Guoliang
and Yang, Jun
and Gama, Joao
and Natwichai, Juggapong
and Tong, Yongxin",
title="Sparse Gradient Compression for Distributed {SGD}",
booktitle="Database Systems for Advanced Applications",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="139--155",
abstract="Communication bandwidth is a bottleneck in distributed machine learning, and limits the system scalability. The transmission of gradients often dominates the communication in distributed SGD. One promising technique is using the gradient compression to reduce the communication cost. Recently, many approaches have been developed for the deep neural networks. However, they still suffer from the high memory cost, slow convergence and serious staleness problems over sparse high-dimensional models. In this work, we propose Sparse Gradient Compression (SGC) to efficiently train both the sparse models and the deep neural networks. SGC uses momentum approximation to reduce the memory cost with negligible accuracy degradation. Then it improves the accuracy with long-term gradient compensation, which maintains global momentum to make up for the information loss caused by the approximation. Finally, to alleviate the staleness problem, SGC updates model weight with the accumulation of delayed gradients at local, called local update technique. The experiments over the sparse high-dimensional models and deep neural networks indicate that SGC can compress 99.99{\%} gradients for every iteration without performance degradation, and saves the communication cost up to 48{\$}{\$}{\backslash}times {\$}{\$}.",
isbn="978-3-030-18579-4"
}

@article{zhao,
  title={Global Momentum Compression for Sparse Communication in Distributed {SGD}},
  author={Shen-Yi Zhao and Yinpeng Xie and Hao Gao and Wu-Jun Li},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.12948}
}

@inproceedings{SCAFFOLD,
  title={{SCAFFOLD}: {S}tochastic Controlled Averaging for On-Device Federated Learning},
  author={Sai Praneeth Karimireddy and Satyen Kale and Mehryar Mohri and Sashank J. Reddi and Sebastian U. Stich and Ananda Theertha Suresh},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020},
}

@article{lin,
  author    = {Yujun Lin and
               Song Han and
               Huizi Mao and
               Yu Wang and
               William J. Dally},
  title     = {Deep Gradient Compression: Reducing the Communication Bandwidth for
               Distributed Training},
  journal   = {CoRR},
  volume    = {abs/1712.01887},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01887},
  archivePrefix = {arXiv},
  eprint    = {1712.01887},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01887},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{basu,
  title={Qsparse-local-{SGD}: Distributed {SGD} with Quantization, Sparsification, and Local Computations},
  author={Debraj Basu and Deepesh Data and Can Karakus and Suhas N. Diggavi},
  booktitle={NeurIPS},
  year={2019}
}

@article{koloskova1,
  author    = {Anastasia Koloskova and
               Tao Lin and
               Sebastian U. Stich and
               Martin Jaggi},
  title     = {Decentralized Deep Learning with Arbitrary Communication Compression},
  journal   = {CoRR},
  volume    = {abs/1907.09356},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.09356},
  archivePrefix = {arXiv},
  eprint    = {1907.09356},
  timestamp = {Tue, 30 Jul 2019 12:52:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-09356},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{koloskova2,
  author    = {Anastasia Koloskova and
               Sebastian U. Stich and
               Martin Jaggi},
  title     = {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed
               Communication},
  journal   = {CoRR},
  volume    = {abs/1902.00340},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00340},
  archivePrefix = {arXiv},
  eprint    = {1902.00340},
  timestamp = {Tue, 21 May 2019 18:03:39 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-00340},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Article{Cnat,
  author        = {Horv\'{a}th, Samuel and Ho, Chen-Yu and Horv\'{a}th, L’udovít and Sahu, Atal Narayan and Canini, Marco and Richt\'{a}rik, Peter},
  title         = {Natural Compression for Distributed Deep Learning},
  journal       = {arXiv:1905.10988},
  year          = {2019},
}

@article{zheng,
  author    = {Shuai Zheng and
               Ziyue Huang and
               James T. Kwok},
  title     = {Communication-Efficient Distributed Blockwise Momentum {SGD} with
               Error-Feedback},
  journal   = {CoRR},
  volume    = {abs/1905.10936},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10936},
  archivePrefix = {arXiv},
  eprint    = {1905.10936},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-10936},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{PCDM,
  author        = {Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
  title         = {Parallel coordinate descent methods for big data optimization},
  journal       = {Mathematical Programming},
  year          = {2016},
  volume        = {156},
  number        = {1-2},
  pages         = {433--484},
  __markedentry = {[richtap:1]},
}

@Article{AjiHea,
  author        = {Aji, Alham Fikri and Heafield, Kenneth},
  title         = {Sparse communication for distributed gradient descent},
  journal       = {arXiv:1704.05021},
  year          = {2017},
}




@article{QuRich16-2,
title = {Coordinate descent with arbitrary sampling II: algorithms and complexity},
author = {Qu, Zheng and Richtárik, Peter},
journal = {Optimization Methods and Software},
pages = {858--884},
volume = {31},
issue = {5},
year = {2016}
}

@article{QuRich16-1,
title = {Coordinate descent with arbitrary sampling I: algorithms and complexity},
author = {Qu, Zheng and Richtárik, Peter},
journal = {Optimization Methods and Software},
pages = {829--857},
volume = {31},
issue = {5},
year = {2016}
}


@article{RichTaka16-2,
title = {Parallel coordinate descent methods for big data optimization},
author = {Richtárik, Peter and Takáč, Martin},
journal = {Mathematical Programming},
pages = {433--484},
volume = {156},
issue = {1-2},
year = {2016}
}


@article{RichTaka14,
title = {Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
author = {Richtárik, Peter and Takáč, Martin},
journal = {Mathematical Programming},
pages = {1--38},
volume = {144},
issue = {2},
year = {2014}
}


@article{RichTaka16,
title = {On optimal probabilities in stochastic coordinate descent methods},
author = {Richtárik, Peter and Takáč, Martin},
journal = {Optimization Letters},
pages = {1233--1243},
volume = {10},
issue = {6},
year = {2016}
}


@article{nesterov2012efficiency,
title = {Efficiency of coordinate descent methods on huge-scale optimization problems},
author = {Nesterov, Yurii},
journal = {SIAM Journal on Optimization},
pages = {341--362},
volume = {22},
issue = {2},
year = {2012}
}


@InProceedings{99KFP,
title = {99\% of Worker-Master Communication in Distributed Optimization Is Not Needed},
author = {Mishchenko, Konstantin and Hanzely, Filip and Richtarik, Peter},
booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
pages = {979--988},
year = {2020},
editor = {Jonas Peters and David Sontag},
volume = {124},
series = {Proceedings of Machine Learning Research},
month = {03--06 Aug},
publisher = {PMLR},
abstract = {In this paper we discuss sparsification of worker-to-server communication in large distributed systems. We improve upon algorithms that fit the following template: a local gradient estimate is computed independently by each worker, then communicated to a master, which subsequently performs averaging. The average is broadcast back to the workers, which use it to perform a gradient-type step to update the local version of the model. We observe that the above template is fundamentally inefficient in that too much data is unnecessarily communicated from the workers to the server, which slows down the overall system. We propose a fix based on a new update-sparsification method we develop in this work, which we suggest be used on top of existing methods. Namely, we develop a new variant of parallel block coordinate descent based on independent sparsification of the local gradient estimates before communication. We demonstrate that with only $m/n$ blocks sent by each of $n$ workers, where $m$ is the total number of parameter blocks, the theoretical iteration complexity of the underlying distributed methods is essentially unaffected. As an illustration, this means that when $n=100$ parallel workers are used, the communication of 99% blocks is redundant, and hence a waste of time. Our theoretical claims are supported through extensive numerical experiments which demonstrate an almost perfect match with our theory on a number of synthetic and real datasets.}
}


@InProceedings{ACD-HanzRich,
title = {Accelerated Coordinate Descent with Arbitrary Sampling and Best Rates for Minibatches},
author = {Hanzely, Filip and Richtarik, Peter},
booktitle = {Proceedings of Machine Learning Research},
pages = {304--312},
year = {2019},
editor = {Kamalika Chaudhuri and Masashi Sugiyama},
volume = {89},
series = {Proceedings of Machine Learning Research},
address = {},
month = {16--18 Apr},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v89/hanzely19a/hanzely19a.pdf},
url = {http://proceedings.mlr.press/v89/hanzely19a.html},
abstract = {Accelerated coordinate descent is a widely popular optimization algorithm due to its efficiency on large-dimensional problems. It achieves state-of-the-art complexity on an important class of empirical risk minimization problems. In this paper we design and analyze an accelerated coordinate descent (\texttt{ACD}) method which in each iteration updates a random subset of coordinates according to an arbitrary but fixed probability law, which is a parameter of the method. While mini-batch variants of \texttt{ACD} are more popular and relevant in practice, there is no importance sampling for \texttt{ACD} that outperforms the standard uniform mini-batch sampling. Through insights enabled by our general analysis, we design new importance sampling for mini-batch \texttt{ACD} which significantly outperforms previous state-of-the-art minibatch \texttt{ACD} in practice. We prove a rate that is at most $\mathcal{O}(\sqrt{\tau})$ times worse than the rate of minibatch \texttt{ACD} with uniform sampling, but can be $\mathcal{O}(n/\tau)$ times better, where $\tau$ is the minibatch size. Since in modern supervised learning training systems it is standard practice to choose $\tau \ll n$, and often $\tau=\mathcal{O}(1)$, our method can lead to dramatic speedups. Lastly, we obtain similar results for minibatch nonaccelerated \texttt{CD} as well, achieving improvements on previous best rates.}
}


@article{QuRich,
title = {Coordinate descent with arbitrary sampling II: expected separable overapproximation},
author = {Zheng,  Qu and Peter,  Richt\'{a}rik},
journal = {Optimization Methods and Software},
pages = {858--884},
volume = {31},
issue = {5},
year = {2016},
doi = {10.1080/10556788.2016.1190361}
}



@unpublished{up_kashin_2020,
  author  = {Safaryan,  Mher and Shulgin,  Egor and Richt\'{a}rik,  Peter},
  title   = {Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor},
  note = {preprint arXiv:2002.08958},
  year    = {2020}
}


@article{GowRic,
title = {Randomized Iterative Methods for Linear Systems},
author = {Gower,  Robert M.  and Richt\'{a}rik,  Peter},
journal = {SIAM Journal on Matrix Analysis and Applications},
pages = {1660--1690},
volume = {36},
issue = {4},
year = {2015}
}

@article{Nsync,
 author = {Richt\'{a}rik,  Peter and Tak\'{a}\v{c},  Martin},
 title = {On optimal probabilities in stochastic coordinate descent methods},
 journal = {Optimization Letters},
 volume = {10},
 year = {2016},
 pages = {1233--1243},
 doi = {https://doi.org/10.1007/s11590-015-0916-1}
} 


@article{nguyen2018sgd,
  title={SGD and Hogwild! Convergence Without the Bounded Gradients Assumption},
  author={Nguyen, Lam M and Nguyen, Phuong Ha and van Dijk, Marten and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:1802.03801},
  year={2018}
}

@article{lacoste2012simpler,
  title={A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}


% ---------------------------------------------- Kashin's representation
@article{HHHSCR,
 abstract = {Due to their hunger for big data, modern deep learning models are trained in parallel, often in distributed environments, where communication of model updates is the bottleneck. Various update compression (e.g., quantization, sparsification, dithering) techniques have been proposed in recent years as a successful tool to alleviate this problem. In this work, we introduce a new, remarkably simple and theoretically and practically effective compression technique, which we call natural compression (NC). Our technique is applied individually to all entries of the to-be-compressed update vector and works by randomized rounding to the nearest (negative or positive) power of two. NC is ``natural'' since the nearest power of two of a real expressed as a float can be obtained without any computation, simply by ignoring the mantissa. We show that compared to no compression, NC increases the second moment of the compressed vector by the tiny factor 9/8 only, which means that the effect of NC on the convergence speed of popular training algorithms, such as distributed SGD, is negligible. However, the communications savings enabled by NC are substantial, leading to 3-4x improvement in overall theoretical running time. For applications requiring more aggressive compression, we generalize NC to natural dithering, which we prove is exponentially better than the immensely popular random dithering technique. Our compression operators can be used on their own or in combination with existing operators for a more aggressive combined effect. Finally, we show that NC is particularly effective for the in-network aggregation (INA) framework for distributed training, where the update aggregation is done on a switch, which can only perform integer computations.},
 author = {Horváth, Samuel and Ho, Chen-Yu and Horváth, Ľudovít and Sahu, Atal Narayan and Canini, Marco and Richtárik, Peter},
 journal = {CoRR},
 month = {May},
 title = {Natural Compression for Distributed Deep Learning},
 url = {http://arxiv.org/abs/1905.10988},
 volume = {abs/1905.10988},
 year = {2019}
}


@article{LyVe,
 author = {Lyubarskii, Yurii and Vershynin, Roman},
 title = {Uncertainty Principles and Vector Quantization},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {July 2010},
 volume = {56},
 number = {7},
 month = jul,
 year = {2010},
 issn = {0018-9448},
 pages = {3491--3501},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/TIT.2010.2048458},
 doi = {10.1109/TIT.2010.2048458},
 acmid = {1840529},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Frame representations, Kashin's representations, frame representations, kashin's representations, restricted isometries, uncertainty principles},
} 

@article{kashin,
 author = {Kashin, Boris S.},
 title = {Diameters of some finite-dimensional sets and classes of smooth functions},
 journal = {Jour. Izv. Akad. Nauk SSSR Ser. Mat.},
 volume = {41},
 number = {2},
 year = {1977},
 pages = {334--351},
 url = {http://mi.mathnet.ru/izv1805},
}


% ---------------------------------------------- signSGD, ICLR2020

@inproceedings{qsgd,
  title={{QSGD}: Communication-efficient {SGD} via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}

@inproceedings{BaHe,
title = {Dissecting {Adam}: The Sign, Magnitude and Variance of Stochastic Gradients},
author = {Balles, Lukas and Hennig, Philipp},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
pages = {404-413},
year = {2018}
}

@inproceedings{BWAA,
title = {sign{SGD}: Compressed Optimisation for Non-Convex Problems},
author = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
pages = {560-569},
volume =  {80},
publisher = {PMLR},
year = {2018}
}

@inproceedings{BZAA,
title = {sign{SGD} with majority vote is communication efficient and fault tolerant},
author = {Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2019}
}

@conference{BoLe,
title = {Large scale online learning},
author = {Bottou, L\'eon and Le Cun, Yann},
booktitle = {Advances in Neural Information Processing Systems},
year = {2003}
}

@conference{CCC,
title = {Stochastic Spectral Descent for Restricted Boltzmann Machines},
author = {Carlson, David and Cevher, Volkan and Carin, Lawrence},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {111-119},
year = {2015}
}

@conference{DHS,
title = {Adaptive subgradient methods for online learning and stochastic optimization},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
pages = {2121–2159},
year = {2011}
}

@conference{GHJY,
title = {Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition},
author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
booktitle = {JMLR: Workshop and Conference Proceedings},
pages = {1-46},
volume = {40},
year = {2015}
}

@conference{GhLa,
title = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
author = {Ghadimi, Saeed and Lan, Guanghui},
booktitle = {SIAM Journal on Optimization},
pages = {2341–2368},
volume = {23(4)},
year = {2013}
}

@conference{KFJ,
title = {Distributed learning with compressed gradients},
author = {Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
booktitle = {arXiv preprint arXiv:1806.06573},
pages = {},
year = {2018}
}

@conference{KiBa,
title = {Adam: A method for stochastic optimization},
author = {Kingma, Diederik and Ba, Jimmy},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2015}
}

@conference{KSH,
title = {Imagenet classification with deep convolutional neural networks},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1097–1105},
year = {2012}
}

@conference{LCCH,
title = {sign{SGD} via zeroth-order oracle},
author = {Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2019}
}


@conference{RKK,
title = {On the convergence of {Adam} and beyond},
author = {Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv},
booktitle = {International Conference on Learning Representations},
pages = {},
year = {2019}
}

@conference{RiBr,
title = {A direct adaptive method for faster backpropagation learning: The {Rprop} algorithm},
author = {Riedmiller, Martin and Braun, Heinrich},
booktitle = {IEEE International Conference on Neural Networks},
pages = {586-591},
year = {1993}
}

@conference{RoMo,
title = {A Stochastic Approximation Method},
author = {Robbins, Herbert and Monro, Sutton},
booktitle = {The Annals of Mathematical Statistics},
pages = {400-407},
volume = {22(3)},
year = {1951}
}

@conference{Sch,
title = {Deep learning in neural networks: An overview},
author = {Schmidhuber, J\"urgen},
booktitle = {Neural networks},
pages = {85–117},
volume = {61},
year = {2015}
}

@conference{SFDLY,
title = {1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech {DNN}s},
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
booktitle = {Fifteenth Annual Conference of the International Speech Communication Association},
pages = {},
year = {2014}
}

@conference{Shev,
title = {On the absolute constants in the Berry–Esseen type inequalities for identically distributed summands},
author = {Shevtsova, Irina},
booktitle = {arXiv preprint arXiv:1111.6554},
pages = {},
year = {2011}
}

@conference{SRB,
title = {Minimizing finite sums with the stochastic average gradient},
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
booktitle = {Mathematical Programming},
pages = {83–112},
volume = {162(1-2)},
year = {2017}
}

@conference{Strom,
title = {Scalable distributed {DNN} training using commodity {GPU} cloud computing},
author = {Strom, Nikko},
booktitle = {Sixteenth Annual Conference of the International Speech Communication Association},
pages = {},
year = {2015}
}

@conference{TiHi,
title = {{RMSprop}},
author = {Tieleman, Tijmen and Hinton, Geoffrey E.},
booktitle = {Coursera: Neural Networks for Machine Learning, Lecture 6.5},
pages = {},
year = {2012}
}

@book{Versh,
place={Cambridge},
series={Cambridge Series in Statistical and Probabilistic Mathematics},
title={High-Dimensional Probability: An Introduction with Applications in Data Science},
DOI={10.1017/9781108231596},
publisher={Cambridge University Press},
author={Vershynin, Roman},
year={2018},
collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@conference{VBS,
title = {Fast and Faster Convergence of {SGD} for Over-Parameterized Models (and an Accelerated Perceptron)},
author = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, PMLR},
pages = {},
volume = {89},
year = {2019}
}

@conference{WSLCPW,
title = {Atomo: Communication-efficient learning via atomic sparsification},
author = {Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
booktitle = {Advances in Neural Information Processing Systems},
pages = {},
year = {2018}
}

@conference{WRSSR,
title = {The marginal value of adaptive gradient methods in machine learning},
author = {Wilson, Ashia and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems},
pages = {4148-4158},
year = {2017}
}

@conference{ZRSKK,
title = {Adaptive methods for nonconvex optimization},
author = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Advances in Neural Information Processing Systems},
pages = {9815-9825},
year = {2018}
}

@conference{Zei,
author = {{Zeiler}, Matthew D.},
title = "{ADADELTA: An Adaptive Learning Rate Method}",
booktitle = {arXiv e-prints, arXiv:1212.5701},
year = "2012"
}



% ---------------------------------------------- initial
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{chang2011libsvm,
  title={{LibSVM}: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{li2020federated,
      title={Federated Optimization in Heterogeneous Networks}, 
      author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
     journal = {arXiv preprint arXiv:1812.06127},
           year={2018},
}

@inproceedings{liu2019double,
      title={A Double Residual Compression Algorithm for Efficient Distributed Learning}, 
      author={Xiaorui Liu and Yao Li and Jiliang Tang and Ming Yan},
      year={2020},
      booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
}

@article{philippenko2021bidirectional,
  title={Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees},
  author={Constantin Philippenko and Aymeric Dieuleveut},
  journal={arXiv preprint arXiv:2006.14591},
  year={2021}
}